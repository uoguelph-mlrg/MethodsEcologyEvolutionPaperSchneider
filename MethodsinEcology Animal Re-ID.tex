%\documentclass[conference]{IEEEtran}
\documentclass[11pt]{article}

\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{wrapfig}

\begin{document}

\title{A Review of Past, Present, and Future Approaches Using Computer Vision for Animal Re-identification from Camera Trap Data}

\author{Stefan Schneider, Graham W. Taylor, Stefan S. Linquist, Stefan C. Kremer}


\maketitle

\textbf{Abstract} - The ability of a researcher to re-identify (re-ID) an individual animal upon re-encounter is a fundamental technique used to answer many basic ecological questions. Ecologists have used a variety of methods for re-ID including, tagging, scarring, DNA analyses, camera traps, and video to test hypotheses about ecosystem function and population dynamics. Tagging animals during mark and recapture studies is the most common method for reliable animal re-ID. However, this method can be laborious, intrusive, and expensive. Camera traps and video is a desirable alternative, requiring less labour, much less intrusion, and prolonged and continuous monitoring into an environment. Despite these advantages, the analyses of camera traps and video for re-ID by humans are criticized for their unavoidable bias and inconsistencies between analyses. While not common practice, for decades ecologists with expertise in computer vision have successfully utilized feature engineering to extract meaningful features from camera trap images to improve the statistical rigor of individual comparisons and remove human bias from their camera trap analyses. Recent years have witnessed the emergence of deep learning systems which learn meaningful features from large data volumes. Current deep learning systems have demonstrated the accurate re-ID of human faces based on image and video data with near perfect accuracy. Despite this success, ecologists have yet to utilize these approaches for animal re-ID. By utilizing novel deep learning methods for object detection and similarity comparisons, ecologists can autonomously extract animals from an image/video data and train deep learning classifiers to re-ID animal individuals beyond the capabilities of a human observer. This methodology will allow ecologists with camera/video trap data to re-identify individuals that exit and re-enter the camera frame. In this review we describe a brief history of camera traps for re-ID, present a collection of computer vision feature engineering methodologies previously used for animal re-ID, provide an introduction to the underlying mechanisms of deep learning relevant to animal re-ID, highlight the success of deep learning methods for human re-ID, describe the few ecological studies currently utilizing deep learning for camera trap analyses, and our predictions for near future methodologies based on the rapid development of deep learning methods.
\newline
\\
\textbf{Index Terms} - Animal Re-Identification, Camera Traps, Computer Vision, Convolutional Networks, Deep Learning, Density Estimation, Object Detection, Population Dynamics

%\twocolumn
\section*{Introduction}

The ability to re-identify individual animals allows for population estimates to be used in a variety of ecological metrics including diversity, evenness, richness, relative abundance distribution, and carrying capacity, which contribute to larger, overarching ecological interpretations of trophic interactions and population dynamics (Whitaker, 1972; Hawley, 1982; Krebs, 1989). Ecologists have used a variety of techniques for re-ID including tagging, scarring, banding, and DNA analyses of hair follicles or feces (Krebs, 1989; Mowat and Strobek, 2000). While accurate, these techniques are laborious for the field research team, intrusive to the animal, and often expensive for the researcher.
\newline
\\
Camera traps provide an alternative approach to animal re-ID by capturing photos in response to motion, or continually recording video, using weather-proof cameras placed at strategic locations (O'Connell, et al., 2010). Compared to more traditional methods of field observations, camera traps are desirable due to their lower cost and reduced workload for field researchers. Camera traps also provide a unique advantage by continually recording the undisturbed behaviours of animals within their environment capturing additional insights such as species specific habitat use and family/pack dynamics (O'Connell et al., 2010). Despite these advantages, the re-identification of animals using camera traps are subject to bias when identified by humans (Rowcliffe et al., 2008). 
\newline
\\
Computer vision approaches standardize the statistical analysis of animal re-ID and remove the potential for human bias. Ecologists have utilized methods of feature engineering to design and implement algorithms that extract meaningful information from images, such as patterns of spots or stripes, that are compared statistically to re-ID individuals. While this method has shown success, feature engineering requires experience designing algorithms for feature extraction, domain expertise to identify the unique characteristics that describe an individual, and uniquely designed algorithm for each species. 
\newline
\\ 
Recent years have witnessed the emergence deep learning systems which make use of large data volumes (Zheng et al., 2016). Modern deep learning systems no longer require hard-coded feature extraction methods, but learn these meaningful features from large amounts of data (LeCun et al., 2015). When considering human re-ID, modern deep learning systems systems outperform all alternatives (Li et al., 2014; Listanti et al., 2015; Martinel et al., 2015; Zheng et al., 2015; Xiao et al., 2016). In recent years, ecologists have since begun utilizing the use of modern deep learning systems for species identification with great success. We believe the future of camera trap analyses for animal re-ID will involve deep learning methods and that ecologists should familiarize themselves with this methodology. Here we present a novel review where we highlight the use of camera traps for re-ID, describe a collection of historic feature engineering approaches used for animal re-ID, provide a brief description of the underlying mechanisms of deep learning systems to educate ecologists unfamiliar with the methodology, highlight the successful implementation of deep learning systems for human re-ID, present a collection of the modern ecological camera trap analyses using deep learning for species identification, and our predictions for near future methodologies based on the rapid development of deep learning methods.
\newline
\\

\section*{A Brief History of Camera Traps and Animal Re-Identification}

Animal photography was first introduced as a way of documenting the rare occurrence of seeing wildlife. While exploring South Africa in 1863, the German professor Gustav Fritsch is credited with the first successful attempt at photographing wild animals (Guggisberg, 1977; Kucera and Barrett, 2011). In 1956, Gysel and Davis published the first work describing an automatic camera trap designed specifically for capturing wildlife. It was a simple system where a camera was pointed at bait, which was wired to the camera, and when the animal ate the bait the camera would trigger (Gysel and Davis, 1956). In 1959, Pearson developed a number of novel techniques for camera traps including camera triggers that were activated by foot plates and the interruption of a red laser, film documentation of animals utilizing a 16mm film movie reel, and animal re-ID in film by uniquely clipping the fur of captured animals (Pearson 1959). Iterations of lighter and more portable cameras were developed over the years by Dodge and Snyder (1960) and Winkler and Adams (1968) for film recordings. In 1984, Seydack performed the first long term camera trap study leaving a camera in the South African rain forest for a month. Seyback (1984) developed his own autonomous trip plate, winding wheel, and flash system for a 35mm camera and repeated the study six times over three years commenting on how the methodology will forever change ecological research. In 1987, Hiby and Jeffrey demonstrated the unique advantage of the discrete nature of camera traps by documenting the Mediterranean monk seal (\textit{Monachus monachus}), a species highly sensitive to human disturbance. In 1991, Carthew and Slater improved upon camera motion detection by introducing a pulse infrared beam as a trigger and, in 1994, Mace et al., iterated again combining microwave motion with passive infrared heat sensors to act as a trigger. In 1994, Mace et al. used their trigger system to provide one of the first population estimates based purely on camera trap data considering the abundance of grizzly bears (\textit{Ursus arctos}). Due to the increased reliability of camera triggers and the decreased cost/size of cameras, camera traps became increasingly popular and more commonly used among ecological researchers. One such area was avian research, where numerous researchers took the opportunity to study nest predators (Laurance and Grant, 1994; Major and Gowing, 1994; Leimgruber et al., 1994; Danielson et al., 1996). For a more thorough and detailed synopsis of the history of camera traps see O'Connell et al. (2012) and/or Kucera and Barrett (2011). 
\newline
\\
Arguably the most influential researcher for camera trap re-ID methods is Karanth who used automated camera traps to photograph tigers (\textit{Panthera tigris}) in Nagarahole, India and estimate the tigers population density under a formal mark and recapture model (Karanth, 1995; Kucera and Barrett, 2011). Since tigers are large, with obvious patterns/markings, humans can visually compare and identify individuals with reliable accuracy. The success of this study led to further experiments conducted in India by Karanth (Karanth and Nichols, 1998; Karanth et al., 2004) and influenced other researchers to follow Karanth's methodology of examining creatures with obvious markings, such as tigers (O'Brien et al., 2003; Kawanishi and Sundquist, 2004), jaguars (\textit{Panthera onca}) (Silver 2004; Silver et al., 2004; Soisale and Cavalcanti, 2006), leopards (\textit{Panthera pardus}) (Henschel and Ray, 2003), and ocelots (\textit{Leopardus pardalis}) (Maffei et al., 2005; Trolle and Kerry, 2005). Karanth et al. has since continued his study of Nagarahole tiger populations, and in 2006 released a 9-year study documented the estimated survival, recruitment, temporary emigration, transience, and rates of population change (Kucera and Barrett, 2011). In 2008, Rowcliffe and Carbone documented a 50\% annual growth in publications using camera trap methods to assess population sizes between 1998 and 2008 and the trend has persisted until 2015 (Burton et al., 2015). 
\newline
\\
Camera traps have become known as a mark and recapture method with reduced manual labour dependencies, cheaper cost, and additional insight into environmental interactions, such as habitat use, family/population dynamics, and general behaviour (Henschel and Ray 2003; Silveira, et al., 2003; Wegge et al., 2004). Despite these advantages, when considering re-ID camera traps are restricted to animals where each individual reliably has a distinct pattern/feature that a human observer can distinguish (Trolle and Kerry, 2005). Relatively few species have natural markings sufficiently variable to be individually recognizable and as a result most camera trap studies focus on spotted and striped felids (Karanth and Nichols 1998; Henschel and Ray 2003; Maffei et al. 2005). This places a limitation on the viable species considered for the methodology and creates a bias where species without individual markings have been under-represented in recent camera trapping research (Trolle 2003; Srbek-Araujo and Chiarello 2005). Foster and Harmsen (2011) identify these limitations and others while reviewing and criticizing 47 camera trap population studies highlighting leniencies in individual identification, sample size/capture probability, camera location/spacing, the size of the study area, and \textit{ad hoc} density estimations. 
\newline
\\
When analysed by humans, camera traps and video are currently subject to unavoidable biases and inconsistencies when re-identifying animal individuals. In order to standardize the analyses of individuals, ecologists with computer vision expertise developed a number of unique and creative feature engineering approaches to extract meaningful information from images of animals for the purpose of re-ID. We present a collection of these approaches here. 

\section*{Computer Vision Methods for Animal Re-Identification}

Computer vision has been used as a tool for animal re-ID for decades. Here we outline the history of computer vision specific to animal re-ID, beginning with the earliest manual feature extraction methods and their transition to more advanced models involving algorithmic feature engineering.  
\newline
\\
The first use of computer vision for animal re-ID was introduced in 1990 by Whitehead, Mizroch et al., and Hiby and Lovell who published collectively in the same journal. Whitehead (1990) considered sperm whales (\textit{Physeter macrocephalus}) using custom software to scan projector slides of a sperm whales fluke onto a digitizer tablet. The user would manually tap the location of a unique characteristic (such as a nick or a scratch), provide a descriptive characteristic, and save the information to a database. When a unseen image is scanned in, the user inputs the descriptive characteristics and the software considers the maximum sum of similarities to return the most similar individual. With a database of 1,015 individuals, 56 were tested returning a 32\% top-1 and 59\% top-10 accuracy. Mizroch et al. (1990) considered humpback whales (\textit{Megaptera novaeangliae}) using customized software where the fluke was uploaded and the user labeled the image considering a 14-sector fluke map selecting from a list of possible markings (Spots, circles, missing, pigment, etc.). Results were compared using a similarity matrix and with a testing set of 30 images the system returned a top-1 accuracy of 43\%. Lastly, Hiby and Lovell (1990) developed a system considering the grey seal (\textit{Halichoerus grypus}) where the user inputs numerous head captures of an animal from the same pose and the system renders a 3-D model of the animal. An individual's representation is captured in the summation of the grey scale pixel values of each "pattern cell" and considering a testing set of 56 images, Hiby and Lovell report a 98\% top-1 accuracy using this model. In 1995, Dufault and Whitehead revisit the pigment and marking changes over time of sperm whales stating their databases must be updated otherwise individuals will be unrecognizable. In 1998, Huele and Haes compared the wavelet transformation of the fluke of sperm whales receiving a 92.0\% accuracy considering 56 images. 
\newline
\\
Using computer vision for re-ID increased in popularity in the 2000s when Kelly (2001) used the same 3-D model approach as Hibby and Lovell (1990) comparing the collective similarity of pattern cells of cheetah (\textit{Acinonyx jubatus}). Kelly (1990) achieved a 97.5\% top-1 accuracy considering 10,000 images, commenting that the technique relies on quality images and should work for many ungulate and cat species. In 2003, Hillman et al., developed an autonomous feature extraction method to capture unique nicks, scratches, and markings from the dorsal fin of six whale and dolphin species. Named \textit{Finscan}, the system localized 300 xy pairs from each fin and determined individual similarity by calculating the Euclidean distance of these pairs and returning the minimum distance to represent similarity (Hillman et al. 2003). Hillman et al. (2003) report accurate classification results of 50\% top-1 and 75\% top-4 accuracy. 
\newline
\\
The complexity of feature extraction increased in 2004 as Ravela and Gamble used a Taylor approximation of local colour intensities, multi-scale brightness histograms, and curvature/orientation to re-ID marbled salamander (\textit{Ambystoma opacum}) individuals using a normalized cross-covariance. Ravela and Gamble constructed a custom housing for their camera trap with four walls, single coloured floor, and a motion camera to capture top down images of salamanders as they entered. A database of 370 individuals were gathered and considering a testing set of 69 queries the method returned a top-1 accuracy of 72\%. In 2005, Arzoumanian et al. explored whale shark (\textit{Rhincodon typus}) re-ID by analyzing the characteristic flank (front dorsal region) spot patterns using an astronomy algorithm for recognizing star patterns (Groth, 1986). The algorithm creates a list of possible triangles from white dots within an image using xy pairs and compares their orientation between images. Arzoumanian et al. (2005) tested 236 images of individuals and report a top-1 accuracy of 90\%, which they claim incorrect results are mainly from poor image quality. In 2007, Ardovini et al. attempted to re-identify elephants \textit{Loxodonta spp.} based on images using a multi-curve matching technique where a spline curve is fit to the mantle of an individual elephant. To re-ID an elephant, the known spline curves were overlaid atop the image of the elephant and the most similar was considered the same individual (Ardovini et al., 2007). The researchers report a 75\% top-1 accuracy utilizing this technique considering 200 testing images and the methodology has been used by the Centre for African Conservation (Ardovini et al., 2007).
\newline
\\
The first computer vision model for animal re-ID capable of generalizing across species was developed in 2007 by Burghardt and Campbell which extracted inherent singularities (spots, lines, etc.) of animal individuals. The technique involves three cameras pointed at a central location to capture a 3-D representation of the animal and extracts features using Haar-like (pixel difference) descriptors. For each feature, an AdaBoost classifier was trained to determine if the single feature was present or not and individual re-ID governed by an ensemble of these features. In 2010, Sherley et al. used this approach on Robben Island, Africa in one of the first works to perform fully autonomous population estimates solely from camera traps considering the African penguin (\textit{Spheniscus demersus}). The system returned an accuracy between 92-97\% in comparison to records from humans on site (Sherley et al., 2010). 
\newline
\\
In 2009, Hiby et al. collaborated with Karanath to explore their 3-D model representation technique considering his long term tiger population studies. By again dividing individuals into pattern cells, re-ID was considered by the summation of the similarity of the cells were considered using a Bayesian posterior probability estimate and tested considering live tigers and poached tiger rugs. Hiby et al. (2009) report a 95\% and 100\% top-1 and top-5 accuracy considering 298 individuals. In 2011, Burghardt and Campbell attempted to differentiate between zebra (Equus zebra) individuals by dividing the image into rows and comparing a histogram representation of pixel values for different individuals. Burghardt and Campbell report a top-1 accuracy of 67\% when considering 17 individuals, and 50\% when considering 85 individuals. The popularized Scale-Invariant Feature Transformation (SIFT) algorithm was used by Bolgar et al. (2012) extract distinctive features, such as colour distribution, from a population of Masai giraffe (\textit{Giraffa camelopardalis tippelskirchi}). Bolgar et al. (2012) compare estimated population size of the giraffe population with manual mark and recapture methodology and claim their model returns a similar population estimate without providing a numeric value. In 2013, Town et al. compared individual similarity of manta ray species (\textit{Manta alfredi} and \textit{Manta birostris} based on images of their ventral side using the contrast-limited adaptive histogram equalization algorithm to enhance local contrasts and then compare colour histograms using a Gaussian filter (Reza, 2004). Considering a 720 images, Town et al. report a 51\% and 68\% top-1 and top-10 accuracies, closing commenting on the challenges of underwater imagery. In 2017, Hughes and Burghardt developed a fully automated contour-based visual shark fin ID system and show that selective feature extraction from shark fins can successfully represent individuality. Using a fin contour feature extraction method creating representations of `fin space', Hughes and Burghardt (2017) propose a local naive Bayes nearest neighbour non-linear model for individual animal recognition. Their approach returns a top-1 accuracy of 82\% for correctly classified shark individuals, and a 91\% top-10 accuracy considering 2,371 images.

\begin{table}[h!]
\centering
\caption{A summary of the results from feature engineering and feedforward network learning methods for animal re-identification}
\begin{tabular}{ l c l c c }
	\multicolumn{5}{ c }{Computer Vision Animal Re-Identification Techniques}\\
	\hline
	Species & Year & \centering{Methodology} & Test Size & \multicolumn{1}{p{2.5cm}}{\centering Top-1 \\ Accuracy (\%)} \\ \hline
	Sperm Whale & 1990 & Database similarity & 56 & 59 \\
	Humpback Whale & 1990 & Database similarity & 30 & 41.4 \\
	Grey Seal & 1990 & 3-D Pattern Cell similarity & 56 & 98.0 \\
	Sperm Whale & 1998 & Wavelet transformations & 56 & 92.0 \\
	Cheetah & 2001 & 3-D Pattern Cell similarity & 10,000 & 97.5 \\
	Whale/Dolphin & 2003 & XY Pair Euclidean Distance & 23-127 & 50.0 \\
	Marbled Salamander & 2004 & Pixel histogram and local colours & 69 & 72.0 \\
	Whale Shark & 2005 & Star pattern recognition & 236 & 90.0 \\
	Elephant & 2007 & Polynomial multi-curve matching & 200 & 75.0 \\
	African penguin & 2010 & Per feature AdaBoost classifer & Pop. Size & 92-97.0 \\
	Tiger & 2009 & 3-D Pattern Cell similarity & 298 & 95.0 \\
	Zebra & 2011 & Pixel histogram similarity & 85 & 50.0 \\
	Giraffe & 2012 & SIFT colour distributions & 568 & 99.3 \\
	Manta Ray & 2013 & Colour histograms & 720 & 51.0 \\
	Green Turtule & 2014 & Feedforward Network & 72 & 95.0 \\
	Shark & 2017 & Naive Bayes Nearest Neighbour & 2371 & 82.0 \\

\end{tabular}
\end{table}

\section*{Deep Learning and Its Success for Human Re-Identification}

While feature engineered methods for animal re-ID has seen success, it requires domain expertise and the design of specific feature extraction algorithms. Modern deep learning systems have shown great success learning the necessary features for re-ID from data., removing the need to hard-code feature engineering methods. Deep Learning as concept originated in the 1940-1960s as cybernetics, was rebranded as connectionism/neural networks between 1980 and 1995, and starting in 2006 rebranded again as deep learning (McColloch and Pitts, 1943; Hebb, 1949; Mynsky and Papert, 1969; Rumelhart et al., 1986; Hinton et al., 2006). Despite a long history, deep learning has seen a resurgence due to improved computation power and the availability of large data sets, both requirements for the model. In recent years deep learning methods have dramatically improved results in the fields of speech recognition, object recognition/detection, drug discovery, genomics, and many others, becoming the standard methodology when approaching problems with large amounts of data (LeCun et al., 2015). Here, we provide a brief description of the underlying mechanism of deep learning as it relates to computer vision and animal re-ID with the intention to familiarize ecologists with the underlying mechanisms of the methodology. The process outlined is an approach known as supervised learning and is the most common form of deep learning (LeCun et al., 2015). Other forms include unsupervised problems (eg. clustering, generative adversarial networks), reinforcement learning (eg. deep Q-networks), compression (autoencoders), among others (Goodfellow et al., 2016). For additional details on deep learning please refer to `Deep Learning' (LeCun et al., 2015), `Deep Learning in Neural Networks: An Overview' (Schmidhuber, 2015), and/or the text `Deep Learning' (Goodfellow et al., 2016).
\newline
\\
Deep learning is a computational framework where a system's parameters are not designed by human engineers but trained from large amounts of data using a general-purpose learning algorithm (LeCun et al., 2015). Intuitively, deep learning systems are organized as a layered web/graph structure where labeled data are submitted as input, and many modifiable scalar variables (known as weights) are summed and multiplied by a non-linear transformations to output a predicted label from a predefined number of possible choices (Figure 1) (Goodfellow et al., 2016). Each training example is fed through the network providing an answer, and based on the results from an objective function (often average answer accuracy across the data set) the weight values are modified by an optimizer (often gradient descent with backpropagation) in an attempt to improve accuracy (Goodfellow et al., 2016). Deep learning systems will often have millions of modifiable weight values stored and with enough data and computation, the underlying relationship between the data and output can be mapped to return accurate results (Krizhevsky et al., 2012; Simonyan and Zisserman, 2015; Szegedy et al., 2015; Kaiming He, 2015). The general term to describe this framework is a neural network, and the specific architecture described here is a feedfoward network (LeCun et al., 2015). In 1991, Hornik proved neural networks are a universal approximator.
\newline
\\
Deep learning systems are best described as multiple processing layers where each layer learns a representation of the data with different levels of abstraction (LeCun et al., 2015). As the data representation passes through each transformation it allows for complex representations to be learned (LeCun et al., 2015). Consider an example data set of many black and white images of animal individuals. The images are initially unravelled into a single vector of pixel values between 0 and 255 and fed into a deep learning system. Using this raw input the first layer is able to learn simple representations of patterns within the data, such as the presence or absence of edges at particular orientations and locations in the image. The second layer will typically represent particular arrangements of edges and open space. The third layer may assemble these edges into larger combinations that correspond to parts of familiar objects, such as the basics of a nose, or eyes, and subsequent layers would detect objects as combinations of these parts, such as a face (LeCun et al., 2015). Based on the combination of larger parts, such as the ears, face, or nose, a learning system is able to correctly classify different individuals with near perfect accuracy when given enough input data (Swanson et al., 2015). A deep learning system can learn the subtle details distinguishing a unique classification, such as a differing antler structure, and ignore large irrelevant variations such as the background, pose, lightning, and surrounding objects (Krizhevsky et al., 2012).
\newline
\\
Due to the near infinite combinations of weight parameters, deep learning models sometimes overfit, learning specific nuances relevant to its training data that are not generally transferable. To detect overfitting and measure a model's performance, before training a model, the data are randomly segregated into a training and testing set where the testing set is never shown to the model during training. The results of the model's overall performance are reported based on its performance on the testing set rather than the training set. Accuracy on the test set is a commonly used metric for performance, however can actually provide a poor representation of a models capabilities when considering data with an unequal distribution of classifications. The F1 Score, introduced by Yang and Liu (1999), is also often included and considers the distribution of correct classifications by quantifying the recall (ratio of correct assignments divided by total number of correct assignments) and precision (ratio of correct assignments divided by the total number of assignments) into a single representation.
\newline
\\
Many of the recent advancements in machine learning have come from improving the architectures of a neural network. One such architecture is the Convolutional Neural Network (CNN), first introduced as Neocognitron in 1979 by Fukushima, which is now the most commonly used architecture for computer vision tasks today (Krizhevsky et al., 2012). CNNs introduce convolutional layers within a network which learn feature maps which represent the spatial similarity of patterns found within the image (LeCun et al., 2015). Examples of feature maps include the presence or absence of lines or colours within an image. Each feature map is governed by a set of `filter banks', which are matrices of scalar values that are learned similar to the standard weights of a network. Intuitively, these feature maps learn to extract the necessary features for a classification task replacing the need for feature engineering. CNNs also introduces max pooling layers, a method that reduces computation and increases robustness by evenly dividing the feature map into regions and returning only the highest activation values (LeCun et al., 2015). A simple CNN will have a two or three convolution layers passed through non-linearity functions, interspersed with two or three pooling layers, ending with fully-connected layers to return a classification output (Figure 2). Machine learning researchers continually experiment with modular architectures of neural networks and four CNN frameworks have standardized as well-performing with differences generally considering computation cost and memory in comparison to accuracy. These networks include AlexNet (Krizhevsky et al., 2012), VGG16 (Simonyan and Zisserman, 2015), GoogLeNet/InceptionNet which introduced the inception module (Szegedy et al., 2015), and ResNet which introduce skip connections (Kaiming He, 2015). These networks range from 7 to 152 layers. A common approach for when training a network for a niche task like animal re-ID is to use the pre-trained weights of one of these four network structures trained on a public data set as initialization parameters, and retrain a model using labeled data of animal individuals (Pan and Yang, 2010). This is approach is known as Transfer Learning (Pan and Yang, 2010).
\newline
\\
The deep learning methods described so far are limited to returning one classification per image, however this is unrealistic for the meaningful analysis of camera trap images. In order to compare the appearance when multiple objects from an image, researchers train object detectors which segregate the image into regions that are passed through a CNN. Three approaches for object detection have recently seen success and grown in popularity. The first is Faster Region-CNN which intelligently segregates the image into a approximately 2000 proposal regions and passes each through a CNN (Ren et al., 2015). The second is YOLO (You-Only-Look-Once) which divides an image into a grid of boxes, and passes each box through the network considering a series of predefined 'anchors' relevant to the predicted shape and size classifications of interest (Redmon et al., 2015). Lastly is Single Shot Multibox Detector which considers a set of default boxes and makes adjustments to these boxes during training (Liu et al., 2016). Object detection methods have an additional objective function evaluation metric known as Intersection over Union (IOU), which returns performance as the area of overlap of the true and predicted regions divided by the entire area of the true and predicted regions (Nowozin, 2014). 
\newline
\\
In 2015 two research teams, Lisanti et al. and Martinel et al., demonstrated the successful capabilities of CNNs on human re-ID using the ETHZ data set, a data set composed of 8580 images of 148 unique individuals taken from mobile platforms, where CNNs saturated the testing set with 99.9\% accuracy after seeing 5 images of an individual. Traditional CNNs for re-ID require large amounts of labeled data for each individual which is an infeasible requirement for animal re-ID. In 1993, Bromley et al. introduced the suitable neural network architecture for this problem, titled a Siamese network, which learns to detect if two input images are similar or dissimilar. When considering re-ID, once trained, Siamese networks require only one labeled input image of an individual in order to accurately re-identify if an individual in a second image is the same. In 2015, Taigman et al. utilized a Siamese network for their system DeepFace which achieved a top-1 accuracy of 92.5\% on the YouTube Faces data set, a data set containing videos of 1,595 individuals (Taigman et al., 2015). In 2016, Schroff et al. introduced FaceNet and a three image triplet loss objective function training similarity using input image as well as a similar and dissimilar image. FaceNet currently holds the highest accuracy on the YouTube Faces data set with a 95.12\% top-1 accuracy (Schroff et al., 2016). 
\newline

\section*{Deep Learning for Camera Trap Analysis of Species and Animal Re-Identification}

Despite the success of deep learning methods for human re-ID, few ecological studies have utilized its advantages. In 2014, Carter et al. publish one of the only works using neural networks for animal re-ID considering their system MYDAS, a tool for green turtle \textit{Chelonia mydas} re-ID. The authors highlight that green turtles have a distinctive features on their shell and collected a variety of photos of 72 individuals including when they were nesting and free swimming for their training and testing set. Their algorithm first preprocesses the image by extracting the shell pattern, converting it to grey scale, unravelling the data into a raw input vector and training a simple feedforward network (Carter et al., 2014). An average model produces an output accuracy of 80-85\% accuracy, but the authors utilize an ensemble approach by training 50 different networks and having each vote for a correct classification. The ensemble approach returns an accuracy of 95\%. MYDAS has been considered a large success and is currently used on Lady Elliot Island in the southern Great Barrier Reef to monitor the green turtle population (Carter et al., 2014).
\newline
\\
Not specific to re-ID, but there have been recent studies comparing the use of deep learning to automate species identification from camera trap analysis. In 2016, Gomez et al. released two papers using deep CNNs for camera trap species recognition. The first compared 8 variations of the established the CNN frameworks AlexNet, VGG, GoogLeNet, and ResNet to train species classification on the Snapshot Serengeti data set, a dataset composed of 3.2 million camera trap images of 60 animal species, achieving a 88.9\% top one and 98.1\% top five accuracy. In 2016, Gomez et al. released a second paper aimed to utilize deep learning to improve low resolution animal species recognition by training deep CNNs on poor quality images. The data was labeled by experts into two data sets, the first classifying between bird and mammal and the second species classification of different mammal species. The authors claim to have reached an accuracy of 97.5\% and 90.35\% on the two classification tasks respectively. 
\newline
\\
In 2017, Norouzzadeh et al., used transfer learning to train six different deep convolutional network architectures (AlexNet, VGG, ResNet-50, ResNet-101, ResNet-152, and GoogLeNet) on a modified Snapshot Serengeti data set removing all images labeled as having multiple species (approximately 5\%), and also removed species with low detection frequency reducing (Norouzzadeh, et al. 2017). Their best model achieved 92.1\% accuracy. The authors emphasize the time saving benefit of the technique and encourage further work into behaviour estimation, gender and age utilizing the data set's additional classifications. In 2018, Schneider et al. addressed the limitation of traditional CNNs returning one classification output per image by comparing object detector model Faster R-CNN for quantifying numerous species classifications from images returning an accuracy of 93.0\% and 76.7\% on the Reconyx Camera Trap, a dataset of 936 images of 20 species, and refined Gold Standard Snapshot Serengeti dataset respectively. The authors cite high class imbalance as the limiting factor for Snapshot Serengeti model.

\section*{Near Future Techniques for Animal Re-Identification}
Ecologists familiar with techniques in computer vision have utilized many novel and creative methods to extract features for animal re-ID, but none have considered modern deep learning methods, such as CNNs or Siamese networks. Many of the animal detection studies listed above state the advantages their methods provide for autonomous species re-identification using computer vision including: increased accuracy, reduced biases, and reduced labour costs, however these advantages are limited to those with the domain knowledge and computer science expertise to design feature extraction algorithms. By considering modern deep learning approaches, ecologists can utilize these advantages without the requirement of hand-coded feature extraction methods by training a neural network to learn these features from large amounts of data. 
\newline
\\
While limited within the field of ecology, deep learning approaches have shown great success re-identifying human individuals considering only a single image for reference when using a network designed for similarity comparisons when trained on large amounts of data. We foresee the creation of labeled datasets for numerous animal individuals as the largest hurdle for deep learning approaches for animal re-ID. Our proposed approach for data collection would be to utilize environments with known ground truths for individuals, such as zoos or camera traps in combination with individuals being tracked by GPS, to build the datasets. We would recommend using video wherever possible to gather the greatest number of images for a given encounter with an individual. To reduce the labour for manual cropped image extraction, we would recommend training an object detector and combine it with simple tracking to autonomously extract cropped and labeled images of individuals from video. We encourage researchers with images of labeled animal images to make these datasets publicly available to further the research in this field.
\newline
\\
While deep learning approaches are able to generalize to examples similar to those seen during training, we foresee challenge scenarios where environmental, positional, and rapid changes to an individual between sightings, may cause difficulties. Environmental difficulties may include challenging weather conditions, such as heavy rain, or extreme lighting/shadows. A possible solution may be to these concerns limit re-ID to optimal weather conditions. A positional challenge may occur if an individual were to enter the camera frame at extremely near or far distances. To solve this, one could limit animals to a certain range from the camera before considering it for re-ID. Lastly, a challenge may be if an individuals appearance were to change dramatically between sightings, such as being injured or the rapid growth of a youth. While a network would be robust to such changes given training examples, this would require examples be available as training data. 
\newline
\\
The development of a computer vision methods for animal re-ID have aided in the autonomous and unbiased population estimation. If deep learning systems can demonstrate accurate animal re-ID at a similar success to humans, one could utilize these methodologies to create systems that autonomously extract a variety of ecological metrics such as diversity, evenness, richness, relative abundance distribution, carrying capacity, and trophic function contributing to larger overarching ecological interpretations of trophic interactions and population dynamics.
\newline
\\
\section*{Conclusion}
Population estimates are the underlying metric for many fundamental ecological questions and relies on the accurate re-identification of animals. Camera and video data have become increasingly common due to their relatively inexpensive method of data collection however they are criticized for their unreliability and bias towards animals with obvious markings. Feature engineering methods for computer vision have shown success re-identifying animal individuals and removing biases from these analyses, however these methods require algorithms designed for feature extraction. Deep learning provides a promising alternative for ecologists as it learns these features from large amounts and has shown success for human re-ID. By utilizing deep learning methods for object detection and similarity comparison, ecologists can utilize deep learning methods to autonomously re-identify animal individuals from camera trap data. Such a tool would allow ecologists to automate population estimates.

\section*{References}

Adams, J. D., Speakman, T., Zolman, E., Schwacke, L. H. 2006. "Automating Image Matching, Cataloging, and Analysis for Photo-Identification Research". Aquatic Mammals. 32(3): 374-385
\newline
\\
Ahmed, N., Natarajan, T., Rao, K. R. (1974). Discrete cosine transform. IEEE transactions on Computers, 100(1), 90-93.
\newline
\\
Ardovini, A., Cinque, L., Sangineto. 2007. "Identifying elephant photos by multi-curve matching". Pattern Recognition. 41:1867-1877
\newline
\\
Arzoumanian, Z., Holmberg, J., Norman, B. (2005). An astronomical pattern‐matching algorithm for computer‐aided identification of whale sharks Rhincodon typus. Journal of Applied Ecology, 42(6), 999-1011.
\newline
\\
Bolgar, D. T., Morrison, T. A., Vance, B., Lee, D., Farid, H. 2012. "A computer-assisted system for photographic mark-recapture analysis". Methods in Ecology and Evolution. 3(5):813-822.
\newline
\\
Bromley, J., Bentz, J. W., Bottou, L., Guyon, I., LeCun, Y., Moore, C., Sackinger, E., Shah, R. 1993. “Signature verification using a siamese ¨time delay neural network,” International Journal of Pattern Recognition and Artificial Intelligence. 7(4): 669–688.
\newline
\\
Burton, A. C., Neilson, E., Moreira, D., Ladle, A., Steenweg, R., Fisher, J. T., Bayne, R., Boutin, S. 2015. "REVIEW: Wildlife camera trapping: a review and recommendations for linking ecological surveys to ecological processes". Journal of Applied Ecology. 52(3): 675-685.
\newline
\\
Carreira, J., Zisserman, A. 2017. "Quo Vaid, Action Recognition? A New Model and the Kinetics Dataset". Computer Vision and Pattern Recognition. arXiv:17095.07750.
\newline
\\
Carter, S. J., Bell, I. P., Miller, J. J., Gash, P. P. (2014). "Automated marine turtle photograph identification using artificial neural networks, with application to green turtles". Journal of Experimental Marine Biology and Ecology, 452, 105-110.
\newline
\\
Carthew, S. M., Slater, E. 1991. "Monitoring animal activity with automated photography". The Journal of Wildlife Management. 689-692.
\newline
\\
Chapman, D.G. 1951. "Some properties of the hypergeometric distribution with applications to zoological sample censuses". Berkeley, University of California Press.
\newline
\\
Danielson, W. R., Degraaf, R. M., Fuller, T. K. (1996). "An Inexpensive Compact Automatic Camera System for Wildlife Research." Journal of Field Ornithology, 414-421.
\newline
\\
Dell, A. I., Bender, J. A., Branson, K., Couzin, I. D., de Polavieja, G. G., Noldus, L. P., ... Brose, U. (2014). Automated image-based tracking and its application in ecology. Trends in ecology \& evolution, 29(7), 417-428.
\newline
\\
Dodge, W., E. Synder, D., P. 1960. "An automatic camera device for recording wildlife activity". The Journal of Wildlife Management. 24(3): 1960.
\newline
\\
Dufault, S., Whitehead, H. A. L. (1995). An assessment of changes with time in the marking patterns used for photoidentification of individual sperm whales, Physeter macrocephalus. Marine Mammal Science, 11(3), 335-343.
\newline
\\
Foster, R. J., Harmen, B. J. 2011. "A critique of density estimation from camera-trap data". The Journal of Wildlife Management. 76(2):224-236.
\newline
\\
Fukushima, K. 1979. "Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position- Neocognitron". Eletron. \& Commun. Japan. 62(10): 11-18.
\newline
\\
Girshick, R., Donahue, J., Darrell, T., Malik, J. 2014. Region-based convolutional networks for accurate object detection and segmentation. TPAMI. 
\newline
\\
Girshick, R. (2015). "Fast R-CNN". in ICCV 2015. arXiv:1504.08083.
\newline
\\
Goodfellow, I., Bengio, Y., Courville, A. 2016. "Deep Learning". MIT Press.
\newline
\\
Gong, S., Xiang, T. 2011. "Person re-identification". Visual Analysis of Behaviour. 301-313. Springer London.
\newline
\\
Groth, E. J. (1986). A pattern-matching algorithm for two-dimensional coordinate lists. The astronomical journal, 91, 1244-1248.
\newline
\\
Guggisberg, C. A. W. 1977. "Early Wildlife Photographers by C. A. W. Guggisberg). Taplinger Pub Co. First edition.
\newline
\\
Gysel, L. W., Davis, E. M. 1956. "A simple automatic photographic unit for wildlife research". The Journal of Wildlife Management. 20(4): 451-453
\newline
\\
Hawley, H. A. 1982. "Ecology". 159-63.
\newline
\\
Hebb, D. O. 1949. "The organization of behavior: A neuropsychological approach". John Wiley \& Sons. 
\newline
\\
Henschel, P., Ray, J. 2003. "Leopards in African rainforests: survey and monitoring techniques". Wildlife Conservation Society.
\newline
\\
Hiby, A. R., Jeffery, J. S. 1987. "Census techniques for small populations, with special reference to the Mediterranean monk seal". Symposium of Zoological Society of London. 58: 193-210.
\newline
\\
Hiby, L., Lovell, P. (1990). Computer aided matching of natural markings: a prototype system for grey seals. Report of the International Whaling Commission, 12, 57-61.
\newline
\\
Hiby, L., Lovell, P., Patil, N., Kumar, N. S., Gopalaswamy, A. M., Karanth, K. U. (2009). A tiger cannot change its stripes: using a three-dimensional model to match images of living tigers and tiger skins. Biology letters, rsbl-2009.
\newline
\\
Hillman, G. R., Wursig, B., Gailey, G. A., Kehtarnavaz, N., Drobyshevsky, A., Araabi, B. N., Tagare, H. D., Weller, D. W. 2003. "Computer-assisted photo-identification of individual marine vertebrates: a multi-species system". Aquatic Mammals. 29(1):117-123
\newline
\\
Hinton, G. E., Osindero, S., Teh, Y. W. 2006. "A fast learning algorithm for deep belief nets". Neural computation. 18(7):1527-1554
\newline
\\
Hornik, K. 1991. "Approximation capabilities of multilayer feedforward networks". Neural Networks. 4(2): 251-257.
\newline
\\
Huele, R. and Udo de Haes, H. (1998) Identification of individual sperm whales by wavelet transform of the trailing edge of the flukes. Marine Mammal Science 14, 143–145.
\newline
\\
Hughes, B., Burghardt, T. 2016. "Automated Visual Fin Identification of Individual Great White Sharks". International Journal of Computer Vision. arXiv:1609.06323v2.
\newline
\\
Jaccard, P. 1912. "The Distribution of the flora in the alpine zone". New Phytologist, 11: 37-50. 
\newline
\\
Jolly, G. M. 1965. "Explicit estimates from capture-recapture data with both death and immigration-stochastic model". Biometrika. 52(1/2): 225-247.
\newline
\\
Karanth, K. U. 1995. "Estimating tiger Panthera tigris populations from camera-trap data using capture-recapture models". Biological conservation. 71(3):333-338.
\newline
\\
Karanth, K. U., Nichols, J. D. 1998. "Estimation of tiger densities in India using photographic captures and recaptures". Ecology. 79(8): 2852-2862.
\newline
\\
Karanth, K. U., Nichols, J. D., Kumar, N. S., Link, W. A., Hines, J. E. 2004. "Tigers and their prey: predicting carnivore densities from prey abundance". Proceedings of the National Academy of Sciences of the United States of America. 101(14): 4853-4858.
\newline
\\
Karanth, K. U., Nichols, J. D., Kumar, N. S., Hines, J. E. 2006. "Assessing tiger population dynamics using photographic capture-recapture sampling". Ecology. 87:2925-2937.
\newline
\\
Kawanishi, K., Sundquist, M. E. 2004. "Conservation status of tigers in a primary rainforest of Peninsular Malaysia". Biological Conservation. 120:333-348.
\newline
\\
Kelly, M. J. (2001). Computer-aided photograph matching in studies using individual identification: an example from Serengeti cheetahs. Journal of Mammalogy, 82(2), 440-449.
\newline
\\
Krebs, C. J. 1999. "Ecological Methodology (2nd ed.). ISBN 9780321021731.
\newline
\\
Krizhevsky, A., Sutskever, I., Hinton, G., E. 2012. “Imagenet classification with deep convolutional neural networks,” in Advances in Neural Information Processing Systems, pp. 1097–1105.
\newline
\\
Kucera, T. E., Barrett, R., H. 2011. "A history of camera trapping". Camera traps in animal ecology. Springer Japan. 9-26.
\newline
\\
Laurance, W. F., Grant, J. D. (1994). "Photographic identification of ground-nest predators in Australian tropical rainforest". Wildlife Research, 21(2), 241-247.
\newline
\\
LeCun, Y., Bengio, Y., Hinton, G. 2015. "Deep learning". Nature. 521:436-444.
\newline
\\
Leimgruber, P., McShea, W. J., Rappole, J. H. (1994). "Predation on artificial nests in large forest blocks". The Journal of wildlife management, 254-260.
\newline
\\
Li, W., Zhao, R., Xiao, T., Wang, X. 2014. “Deepreid: Deep filter pairing neural network for person re-identification.” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 152–159.
\newline
\\
Li, Y. 2015. "Human re-identification in real-world surveillance camera networks." Doctoral dissertation, Rensselaer Polytechnic Institute.
\newline
\\
Lincoln, Frederick C. 1921. "The History and Purposes of Bird Banding". The Auk. 38(2):217–228. doi:10.2307/4073884. Retrieved 21 May 2013.
\newline
\\
Lisanti, G., Masi, I., Bagdanov, A. D., Del Bimbo, A. 2015. "Person re-identification by iterative re-weighted spare ranking". IEEE transactions on pattern analysis and machine intelligence. 37(8): 16290-1642.
\newline
\\
Mace, R. D., Minta, S. C., Manley, T., Aune, K. E. 1994. "Estimating grizzly bear population size using camera sightings". Wildlife Society Bulletin. 22:74-83.
\newline
\\
Maffei, L., Noss, A. J., Cuellar, E., Rumiz, D.I. 2005. "Ocelot (\textit{Felis pardalis}) population densities, activity and ranging beahviour in the dry forests of eastern Bolivia: data from camera trapping". Journal of Tropical Ecology. 21(3): 349-353.
\newline
\\
Major, R. E., Gowing, G. (1994). An inexpensive photographic technique for identifying nest predators at active nests of birds. Wildlife Research, 21(6), 657-665.
\newline
\\
Martinel, N., Das, A., Micheloni, C., Roy-Chowdhury, A. K. 2015. "Re-identification in the function space of feature warps". IEEE transactions on pattern analysis and machine intelligence. 37(8): 1656-1669.
\newline
\\
Mizroch, S. A., Beard, J. A., Lynde, M. (1990). Computer assisted photo-identification of humpback whales. Report of the International Whaling Commission, 12, 63-70.
\newline
\\
Mowat, G., Strobeck, C. 2000. "Estimating population size of grizzly bears using hair capture, DNA profiling, and mark-recapture analysis". The Journal of wildlife management. 183-193.
\newline
\\
Mynsky, M., Paper, S. 1969. "Perceptrons". MIT Press, Cambridge MA. 
\newline
\\
Nowozin, S. 2014. "Optimal Decisions from Probabilistic Models: the Intersection-over-Union Case". IEEE Conference on Computer Vision and Pattern Recognition. 548-555.
\newline
\\
O'Brien, T. G., Kinnaird, M. F., Wibisono, H. T. 2003. "Crouching tigers, hidden prey: Sumatran tiger and prey populations in a trophical forest landscape". Animal Conservation forum. Cambridge University Press. 6(2):131-139.
\newline
\\
O'Connell, A. F., Nichols, J. D., Karanath, K. U. 2010. "Camera traps in animal ecology: method and analyses". Spring Science \& Business Media.
\newline
\\
Pan, S. J., Yang, Q. 2010. "A survey on transfer learning". IEEE Transactions on knowledge and data engineering. 22(10): 1345-1359.
\newline
\\
Pearson, O. P. 1959. "A traffic survey of Microtus-Reithrodontomys runways". Journal of Mammalogy. 40(2): 169-180.
\newline
\\
Pledger, S. Pollock, K. H., Norris, J. L. 2003. "Open Capture-Recapture Models with Heterogeneity: I. Cormack-Jolly-Seber Model". Biometrics. 59: 786-794.
\newline
\\
Ravela, S., Gamble, L. (2004, January). On recognizing individual salamanders. In Proceedings of Asian Conference on Computer Vision, Ki-Sang Hong and Zhengyou Zhang, Ed. Jeju, Korea (pp. 742-747).
\newline
\\
Ren, S., He, K., Girshick, R., Sun, J. 2016. "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. arXiv:1506.01497.
\newline
\\
Rowcliffe, J. M., Carbone, C. 2008. "Surveys using camera traps: are we looking to a brighter future?" Animal Conservation. 11(3): 185-186.
\newline
\\
Rowcliffe, J. M., Field, J., Turvey, S. T., Carbone, C. 2008. "Estimating animal density using camera traps without the need for individual recognition". Journal of Applied Ecology. 45(4): 1228-1236.
\newline
\\
Rumelhart, D. E., Hinton G. E., Williams, R. J. 1986. "Learning representations by back-propagating errors". Nature. 323: 533-536.
\newline
\\
Schmidhuber, J. 2015. "Deep Learning in Neural Networks: An Overview". Neural Networks: The Official Journal of the International Neural Network Society. 61:85-117.
\newline
\\
Seber, G. A. F. 1965. "A note on the multiple-recapture census". Biometrika. 52(1/2): 249-259.
\newline
\\
Seydack, A. H. 1984. "Application of a photo-recording device in the census of larger rain-forest mammals". South African Journal of Wildlife Research-24-month delayed open access. 14(1): 10-14.
\newline
\\
Sherley, R. B., Burghardt, T., Barham, P. J., Campbell, N., Cuthill, I. C. (2010). Spotting the difference: towards fully-automated population monitoring of African penguins Spheniscus demersus. Endangered Species Research, 11(2), 101-111.
\newline
\\
Silveira, L., Jacomo, A. T. A., Diniz-Filho, J. A. F. 2003. "Camera trap, line transect census and track surveys: a comparative evaluation". Biological Conservation. 114(3):351-355.
\newline
\\
Silver, S. C. 2004. "Assessing jaguar abundance using remotely triggered cameras". Wildlife Conservation Society. 1-27.
\newline
\\
Silver, S.C., Ostro, L. E. T., Marsh, L. K., Maffei, L., Noss, A. J., Kelly, M. J., Wallace, R. B., Gomez, H., Ayala, G. 2004. "The use of camera traps for estimating jaguar \textit{Panthera onca} abundance and density using capture/recapture analysis". Oryx. 38:148-154.
\newline
\\
Simonyan, K., Zisserman, A. 2015. "Very Deep Convolutional Networks for Large-Scale Image Recognition". arXIv preprint. 1409.1556.
\newline
\\
Southwood, T. R. E., Henderson, P. 2000. "Ecological Methods (3rd ed.)". Oxford: Blackwell Science.
\newline
\\
Srbek-Araujo, A. C., Chiarello A. G. 2005. "Is camera-trapping an efficient method of surveying mammals in Neotropical forests? A case study in south-eastern Brazil". Journal of Tropical Ecology. 21(1):121-125.
\newline
\\
Swanson, A., Kosmala, M., Lintott, C., Simpson, R., Smith, A., Packer, C. 2015. "Snapshot Serengeti, high-frequency annotated camera trap images of 40 mammalian species in an African savanna". Scientific Data 2: 150026.
\newline
\\
Szegedy, C., Liu, W., Jia, Y., Sermanet, P. 2015. "Going deeper with convolutions". Proceedings of the IEEE conference on computer vision and pattern recognition.
\newline
\\
Taigman, Y., Yang, M., Ranzato, M. A., Wolf, L. 2015. "Deepface: Closing the gap on human-level performance in face verification". Proceedings of the IEEE conference on computer vision and pattern recognition. 1701:1708.
\newline
\\
Trolle, M. 2003. "Mammal survey in the southeastern Pantanal, Brazil". Biodiversity and Conservation. 12(4):823-836.
\newline
\\
Trolle, M., Kerry, M. 2005. "Camera-trap study of ocelot and other secretive mammals in the northern Pantanal". Mammalia. 69:409-416.
\newline
\\
Uijlings, J. R. Van De Sande, K. E., Gevers T., Smeulders, A. W. 2013. "Selective search for object recognition." International journal of computer vision. 104.2, 154-171.
\newline
\\
Wegge, P., Pokheral, C. P., Jnawali, S. R. 2004. "Effects of trapping effort and trap shyness on estimates of tiger abundance from camera trap studies". Animal Conservation forum. 7(3): 251-256.
\newline
\\
Witkin, A. (1984, March). Scale-space filtering: A new approach to multi-scale description. In Acoustics, Speech, and Signal Processing, IEEE International Conference on ICASSP'84. (Vol. 9, pp. 150-153). IEEE.
\newline
\\
Whitehead, H. (1990). Computer assisted individual identification of sperm whale flukes. Report of the International Whaling Commission, 12, 71-77.
\newline
\\
Winkler, W. G., Adams, D. B. 1968. "An automatic movie camera for wildlife photography". The Journal of Wildlife Management. 949-952.
\newline
\\
Xiao, T., Li, S., Wang, B., Lin, L., Wang, X. 2016. "End-to-End Deep Learning for Person Search". arXiv preprint arXiv:1604:01850.
\newline
\\
Yang, Y., Liu, X. 1999. "A re-examination of text categorization methods". Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval. 42-49.
\newline
\\
Zheng, L., Bie, Z., Sun, Y., Wang, J., Su, C., Wang, S., Tian Q. 2016. "Mars: A video benchmark for large-scale person re-identification". In European Conference on Computer Vision. 868-884. Springer International Publishing.
\newline
\\
Zheng, L., Shen, L., Tian, L., Wang, S., Wang, J., Tian Q. 2015. "Scalable person re-identification: A benchmark". Proceedings of the IEEE International Conference on Computer Vision. 1116-1124.

\end{document}



%\begin{table}[h]
%\centering
%\caption{A summary of the results from feature engineering and deep learning methods for animal re-identification}
%\begin{tabular}{ l l c l c c }
%	\multicolumn{6}{ c }{Computer Vision Animal Re-Identification Techniques}\\
%	\hline
%	Species & Scientific Name & Year & Methodology & Test Size & \multicolumn{1}{p{1.25cm}}{\centering Top-1 \\ Accuracy (\%)} \\ \hline
%	Sperm Whale & \textit{Physeter macrocephalus} & 1990 & Database similarity & 56 & 59 \\
%	Humpback Whale & \textit{Megaptera novaeangliae} & 1990 & Database similarity & 30 & 41.4 \\
%	Grey Seal & \textit{Halichoerus gryphus} & 1990 & 3-D Pattern Cell similarity & 56 & 98.0 \\
%	Sperm Whale & \textit{Physeter macrocephalus} & 1998 & Wavelet transformations & 56 & 92.0 \\
%	Cheetah & \textit{Acinonyx jubatus} & 2001 & 3-D Pattern Cell similarity & 10,000 & 97.5 \\
%	Whale/Dolphin & \textit{6 species} & 2003 & XY Pair Euclidean Distance & 23-127 & 50.0 \\
%	Marbled Salamander & \textit{Ambystoma opacum} & 2004 & Pixel histogram and local colours & 69 & 72.0 \\
%	Whale Shark & \textit{Rhincodon typus} & 2005 & Star pattern recognition & 236 & 90.0 \\
%	Elephant & \textit{Loxodonta spp} & 2007 & Polynomial multi-curve matching & 200 & 75.0 \\
%	African penguin & \textit{Spheniscus demersus} & 2010 & Per feature AdaBoost classifer & Pop. Size & 92-97.0 \\
%	Tiger & \textit{Panthera tigris} & 2009 & 3-D Pattern Cell similarity & 298 & 95.0 \\
%	Zebra & \textit{Equus zebra} & 2011 & Pixel histogram similarity & 85 & 50.0 \\
%	Giraffe & \textit{Giraffa camelopardalis} & 2012 & SIFT colour distributions & 568 & 99.3 \\
%	Manta Ray & \textit{Manta birostris} & 2013 & Colour histograms & 720 & 51.0 \\
%	Green Turtule & \textit{Chelonia mydas} & 2014 & Feedforward Network & 72 & 95.0 \\
%	Shark & \textit{Selachimorpha} & 2017 & Naive Bayes Nearest Neighbour & 2371 & 82.0 \\

%\end{tabular}
%\end{table}